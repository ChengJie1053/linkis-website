"use strict";(self.webpackChunklinkis_web_apache=self.webpackChunklinkis_web_apache||[]).push([[62894],{3905:(e,t,n)=>{n.d(t,{Zo:()=>d,kt:()=>k});var a=n(67294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function l(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?l(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):l(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function o(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},l=Object.keys(e);for(a=0;a<l.length;a++)n=l[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);for(a=0;a<l.length;a++)n=l[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var s=a.createContext({}),p=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},d=function(e){var t=p(e.components);return a.createElement(s.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,l=e.originalType,s=e.parentName,d=o(e,["components","mdxType","originalType","parentName"]),m=p(n),k=r,g=m["".concat(s,".").concat(k)]||m[k]||u[k]||l;return n?a.createElement(g,i(i({ref:t},d),{},{components:n})):a.createElement(g,i({ref:t},d))}));function k(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var l=n.length,i=new Array(l);i[0]=m;var o={};for(var s in t)hasOwnProperty.call(t,s)&&(o[s]=t[s]);o.originalType=e,o.mdxType="string"==typeof e?e:r,i[1]=o;for(var p=2;p<l;p++)i[p]=n[p];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},10564:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>i,default:()=>u,frontMatter:()=>l,metadata:()=>o,toc:()=>p});var a=n(87462),r=(n(67294),n(3905));const l={title:"Spark Engine",sidebar_position:1},i=void 0,o={unversionedId:"engine-usage/spark",id:"engine-usage/spark",title:"Spark Engine",description:"This article mainly introduces the installation, use and configuration of the Spark engine plugin in Linkis.",source:"@site/docs/engine-usage/spark.md",sourceDirName:"engine-usage",slug:"/engine-usage/spark",permalink:"/docs/1.3.2/engine-usage/spark",draft:!1,editUrl:"https://github.com/apache/linkis-website/edit/dev/docs/engine-usage/spark.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{title:"Spark Engine",sidebar_position:1},sidebar:"tutorialSidebar",previous:{title:"Overview",permalink:"/docs/1.3.2/engine-usage/overview"},next:{title:"Hive Engine",permalink:"/docs/1.3.2/engine-usage/hive"}},s={},p=[{value:"1. Preliminary work",id:"1-preliminary-work",level:2},{value:"1.1 Engine installation",id:"11-engine-installation",level:3},{value:"1.2 Environment verification",id:"12-environment-verification",level:3},{value:"2. Engine plugin installation default engine",id:"2-engine-plugin-installation-default-engine",level:2},{value:"3. Using the <code>spark</code> engine",id:"3-using-the-spark-engine",level:2},{value:"3.1 Submitting tasks via <code>Linkis-cli</code>",id:"31-submitting-tasks-via-linkis-cli",level:3},{value:"3.2 Submitting tasks through <code>Linkis SDK</code>",id:"32-submitting-tasks-through-linkis-sdk",level:3},{value:"3.3 Submitting tasks with <code>Restful API</code>",id:"33-submitting-tasks-with-restful-api",level:3},{value:"4. Engine configuration instructions",id:"4-engine-configuration-instructions",level:2},{value:"4.1 Default Configuration Description",id:"41-default-configuration-description",level:3},{value:"4.2 Queue resource configuration",id:"42-queue-resource-configuration",level:3},{value:"4.3 Configuration modification",id:"43-configuration-modification",level:3},{value:"4.3.1 Management Console Configuration",id:"431-management-console-configuration",level:4},{value:"4.3.2 Task interface configuration",id:"432-task-interface-configuration",level:4},{value:"4.4 Engine related data sheet",id:"44-engine-related-data-sheet",level:3},{value:"5. introduction for data_calc\uff08data calculate\uff09",id:"5-introduction-for-data_calcdata-calculate",level:2},{value:"5.1 Mode Array",id:"51-mode-array",level:3},{value:"5.2 Mode Group",id:"52-mode-group",level:3},{value:"5.3 Plugin type description",id:"53-plugin-type-description",level:3},{value:"5.3.1 Source configuration",id:"531-source-configuration",level:4},{value:"5.3.1.1 file",id:"5311-file",level:5},{value:"5.3.1.2 jdbc",id:"5312-jdbc",level:5},{value:"5.3.1.3 managed_jdbc",id:"5313-managed_jdbc",level:5},{value:"5.3.2 Transform Configuration",id:"532-transform-configuration",level:4},{value:"5.3.2.1 sql",id:"5321-sql",level:5},{value:"5.3.3 Sink Configuration",id:"533-sink-configuration",level:4},{value:"5.3.3.1 hive",id:"5331-hive",level:5},{value:"5.3.3.2 jdbc",id:"5332-jdbc",level:5},{value:"5.3.3.3 managed_jdbc",id:"5333-managed_jdbc",level:5},{value:"5.3.3.4 file",id:"5334-file",level:5},{value:"5.4 Exemples",id:"54-exemples",level:3}],d={toc:p};function u(e){let{components:t,...l}=e;return(0,r.kt)("wrapper",(0,a.Z)({},d,l,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("p",null,"This article mainly introduces the installation, use and configuration of the ",(0,r.kt)("inlineCode",{parentName:"p"},"Spark")," engine plugin in ",(0,r.kt)("inlineCode",{parentName:"p"},"Linkis"),"."),(0,r.kt)("h2",{id:"1-preliminary-work"},"1. Preliminary work"),(0,r.kt)("h3",{id:"11-engine-installation"},"1.1 Engine installation"),(0,r.kt)("p",null,"If you wish to use the ",(0,r.kt)("inlineCode",{parentName:"p"},"spark")," engine on your server, you need to ensure that the following environment variables are set correctly and that the engine's starting user has these environment variables."),(0,r.kt)("p",null,"It is strongly recommended that you check these environment variables for the executing user before executing a ",(0,r.kt)("inlineCode",{parentName:"p"},"spark")," job."),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Environment variable name"),(0,r.kt)("th",{parentName:"tr",align:null},"Environment variable content"),(0,r.kt)("th",{parentName:"tr",align:null},"Remarks"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"JAVA_HOME"),(0,r.kt)("td",{parentName:"tr",align:null},"JDK installation path"),(0,r.kt)("td",{parentName:"tr",align:null},"Required")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"HADOOP_HOME"),(0,r.kt)("td",{parentName:"tr",align:null},"Hadoop installation path"),(0,r.kt)("td",{parentName:"tr",align:null},"Required")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"HADOOP_CONF_DIR"),(0,r.kt)("td",{parentName:"tr",align:null},"Hadoop configuration path"),(0,r.kt)("td",{parentName:"tr",align:null},"required")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"HIVE_CONF_DIR"),(0,r.kt)("td",{parentName:"tr",align:null},"Hive configuration path"),(0,r.kt)("td",{parentName:"tr",align:null},"required")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"SPARK_HOME"),(0,r.kt)("td",{parentName:"tr",align:null},"Spark installation path"),(0,r.kt)("td",{parentName:"tr",align:null},"Required")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"SPARK_CONF_DIR"),(0,r.kt)("td",{parentName:"tr",align:null},"Spark configuration path"),(0,r.kt)("td",{parentName:"tr",align:null},"Required")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"python"),(0,r.kt)("td",{parentName:"tr",align:null},"python"),(0,r.kt)("td",{parentName:"tr",align:null},"It is recommended to use anaconda's python as the default python")))),(0,r.kt)("h3",{id:"12-environment-verification"},"1.2 Environment verification"),(0,r.kt)("p",null,"Verify that ",(0,r.kt)("inlineCode",{parentName:"p"},"Spark")," is successfully installed by ",(0,r.kt)("inlineCode",{parentName:"p"},"pyspark")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"pyspark\n\n#After entering the pyspark virtual environment, the spark logo appears, indicating that the environment is successfully installed\nWelcome to\n      ______\n     /__/__ ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/ '_/\n   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.3\n      /_/\n\nUsing Python version 2.7.13 (default, Sep 30 2017 18:12:43)\nSparkSession available as 'spark'.\n")),(0,r.kt)("h2",{id:"2-engine-plugin-installation-default-engine"},"2. Engine plugin installation ",(0,r.kt)("a",{parentName:"h2",href:"/docs/1.3.2/engine-usage/overview"},"default engine")),(0,r.kt)("p",null,"The ",(0,r.kt)("inlineCode",{parentName:"p"},"Spark")," engine plugin is included in the binary installation package released by ",(0,r.kt)("inlineCode",{parentName:"p"},"linkis")," by default, and users do not need to install it additionally."),(0,r.kt)("p",null,"In theory ",(0,r.kt)("inlineCode",{parentName:"p"},"Linkis")," supports all versions of ",(0,r.kt)("inlineCode",{parentName:"p"},"spark2.x")," and above. The default supported version is ",(0,r.kt)("inlineCode",{parentName:"p"},"Spark2.4.3"),". If you want to use another version of ",(0,r.kt)("inlineCode",{parentName:"p"},"spark"),", such as ",(0,r.kt)("inlineCode",{parentName:"p"},"spark2.1.0"),", you just need to modify the version of the plugin ",(0,r.kt)("inlineCode",{parentName:"p"},"spark")," and compile it. Specifically, you can find the ",(0,r.kt)("inlineCode",{parentName:"p"},"linkis-engineplugin-spark")," module, change the value of the ",(0,r.kt)("inlineCode",{parentName:"p"},"<spark.version>")," tag in the ",(0,r.kt)("inlineCode",{parentName:"p"},"maven")," dependency to 2.1.0, and then compile this module separately."),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"/docs/1.3.2/deployment/install-engineconn"},"EngineConnPlugin engine plugin installation")),(0,r.kt)("h2",{id:"3-using-the-spark-engine"},"3. Using the ",(0,r.kt)("inlineCode",{parentName:"h2"},"spark")," engine"),(0,r.kt)("h3",{id:"31-submitting-tasks-via-linkis-cli"},"3.1 Submitting tasks via ",(0,r.kt)("inlineCode",{parentName:"h3"},"Linkis-cli")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},'# codeType correspondence py--\x3epyspark sql--\x3esparkSQL scala--\x3eSpark scala\nsh ./bin/linkis-cli -engineType spark-2.4.3 -codeType sql -code "show databases"  -submitUser hadoop -proxyUser hadoop\n\n# You can specify the yarn queue in the submission parameter by -confMap wds.linkis.yarnqueue=dws\nsh ./bin/linkis-cli -engineType spark-2.4.3 -codeType sql  -confMap wds.linkis.yarnqueue=dws -code "show databases"  -submitUser hadoop -proxyUser hadoop\n')),(0,r.kt)("p",null,"More ",(0,r.kt)("inlineCode",{parentName:"p"},"Linkis-Cli")," command parameter reference: ",(0,r.kt)("a",{parentName:"p",href:"/docs/1.3.2/user-guide/linkiscli-manual"},"Linkis-Cli usage")),(0,r.kt)("h3",{id:"32-submitting-tasks-through-linkis-sdk"},"3.2 Submitting tasks through ",(0,r.kt)("inlineCode",{parentName:"h3"},"Linkis SDK")),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"Linkis")," provides ",(0,r.kt)("inlineCode",{parentName:"p"},"SDK")," of ",(0,r.kt)("inlineCode",{parentName:"p"},"Java")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"Scala")," to submit tasks to ",(0,r.kt)("inlineCode",{parentName:"p"},"Linkis")," server. For details, please refer to ",(0,r.kt)("a",{parentName:"p",href:"/docs/1.3.2/user-guide/sdk-manual"},"JAVA SDK Manual"),".\nFor ",(0,r.kt)("inlineCode",{parentName:"p"},"Spark")," tasks you only need to modify the ",(0,r.kt)("inlineCode",{parentName:"p"},"EngineConnType")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"CodeType")," parameters in ",(0,r.kt)("inlineCode",{parentName:"p"},"Demo"),":"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-java"},'Map<String, Object> labels = new HashMap<String, Object>();\nlabels.put(LabelKeyConstant.ENGINE_TYPE_KEY, "spark-2.4.3"); // required engineType Label\nlabels.put(LabelKeyConstant.USER_CREATOR_TYPE_KEY, "hadoop-IDE");// required execute user and creator\nlabels.put(LabelKeyConstant.CODE_TYPE_KEY, "sql"); // required codeType py,sql,scala\n')),(0,r.kt)("p",null,"Submit task by ",(0,r.kt)("inlineCode",{parentName:"p"},"OnceEngineConn"),"\uff08spark-submit jar\uff09, example: ",(0,r.kt)("inlineCode",{parentName:"p"},"org.apache.linkis.computation.client.SparkOnceJobTest")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-java"},'public class SparkOnceJobTest {\n\n    public static void main(String[] args)  {\n\n        LinkisJobClient.config().setDefaultServerUrl("http://127.0.0.1:9001");\n\n        String submitUser = "linkis";\n        String engineType = "spark";\n\n        SubmittableSimpleOnceJob onceJob =\n                // region\n                LinkisJobClient.once().simple().builder()\n                        .setCreateService("Spark-Test")\n                        .setMaxSubmitTime(300000)\n                        .setDescription("SparkTestDescription")\n                        .addExecuteUser(submitUser)\n                        .addJobContent("runType", "jar")\n                        .addJobContent("spark.app.main.class", "org.apache.spark.examples.JavaWordCount")\n                        // parameter\n                        .addJobContent("spark.app.args", "hdfs:///tmp/test_word_count.txt") // WordCount test file\n                        .addLabel("engineType", engineType + "-2.4.7")\n                        .addLabel("userCreator", submitUser + "-IDE")\n                        .addLabel("engineConnMode", "once")\n                        .addStartupParam("spark.app.name", "spark-submit-jar-test-linkis") // Application Name on yarn \n                        .addStartupParam("spark.executor.memory", "1g")\n                        .addStartupParam("spark.driver.memory", "1g")\n                        .addStartupParam("spark.executor.cores", "1")\n                        .addStartupParam("spark.executor.instance", "1")\n                        .addStartupParam("spark.app.resource", "hdfs:///tmp/spark/spark-examples_2.11-2.3.0.2.6.5.0-292.jar")\n                        .addSource("jobName", "OnceJobTest")\n                        .build();\n        // endregion\n        onceJob.submit();\n        // Temporary network failure will cause exceptions. It is recommended to modify the SDK later. For use at this stage, exception handling is required\n        onceJob.waitForCompleted();\n    }\n}\n')),(0,r.kt)("h3",{id:"33-submitting-tasks-with-restful-api"},"3.3 Submitting tasks with ",(0,r.kt)("inlineCode",{parentName:"h3"},"Restful API")),(0,r.kt)("p",null,"Scripts type includes ",(0,r.kt)("inlineCode",{parentName:"p"},"sql"),"\u3001",(0,r.kt)("inlineCode",{parentName:"p"},"scala"),"\u3001",(0,r.kt)("inlineCode",{parentName:"p"},"python"),"\u3001",(0,r.kt)("inlineCode",{parentName:"p"},"data_calc(content type is json)"),"."),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"/docs/1.3.2/api/linkis-task-operator"},"Restful API Usage")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-http",metastring:"request",request:!0},'POST /api/rest_j/v1/entrance/submit\nContent-Type: application/json\nToken-Code: dss-AUTH\nToken-User: linkis\n\n{\n    "executionContent": {\n        // script content, type: sql, python, scala, json\n        "code": "show databases",\n        // script type: sql, py\uff08pyspark\uff09, scala, data_calc(json)\n        "runType": "sql"\n    },\n    "params": {\n        "variable": {\n        },\n        "configuration": {\n            // spark startup parameters, not required\n            "startup": {\n                "spark.executor.memory": "1g",\n                "spark.driver.memory": "1g",\n                "spark.executor.cores": "1",\n                "spark.executor.instances": 1\n            }\n        }\n    },\n    "source":  {\n        // not required, file:/// or hdfs:///\n        "scriptPath": "file:///tmp/hadoop/test.sql"\n    },\n    "labels": {\n        // pattern\uff1aengineType-version\n        "engineType": "spark-2.4.3",\n        // userCreator: linkis is username\u3002IDE is system that be configed in Linkis\u3002\n        "userCreator": "linkis-IDE"\n    }\n}\n')),(0,r.kt)("h2",{id:"4-engine-configuration-instructions"},"4. Engine configuration instructions"),(0,r.kt)("h3",{id:"41-default-configuration-description"},"4.1 Default Configuration Description"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Configuration"),(0,r.kt)("th",{parentName:"tr",align:null},"Default"),(0,r.kt)("th",{parentName:"tr",align:null},"Required"),(0,r.kt)("th",{parentName:"tr",align:null},"Description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"wds.linkis.rm.instance"),(0,r.kt)("td",{parentName:"tr",align:null},"10"),(0,r.kt)("td",{parentName:"tr",align:null},"No"),(0,r.kt)("td",{parentName:"tr",align:null},"Maximum number of concurrent engines")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"spark.executor.cores"),(0,r.kt)("td",{parentName:"tr",align:null},"1"),(0,r.kt)("td",{parentName:"tr",align:null},"No"),(0,r.kt)("td",{parentName:"tr",align:null},"Number of spark executor cores")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"spark.driver.memory"),(0,r.kt)("td",{parentName:"tr",align:null},"1g"),(0,r.kt)("td",{parentName:"tr",align:null},"no"),(0,r.kt)("td",{parentName:"tr",align:null},"maximum concurrent number of spark executor instances")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"spark.executor.memory"),(0,r.kt)("td",{parentName:"tr",align:null},"1g"),(0,r.kt)("td",{parentName:"tr",align:null},"No"),(0,r.kt)("td",{parentName:"tr",align:null},"spark executor memory size")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"wds.linkis.engineconn.max.free.time"),(0,r.kt)("td",{parentName:"tr",align:null},"1h"),(0,r.kt)("td",{parentName:"tr",align:null},"No"),(0,r.kt)("td",{parentName:"tr",align:null},"Engine idle exit time")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"spark.python.version"),(0,r.kt)("td",{parentName:"tr",align:null},"python2"),(0,r.kt)("td",{parentName:"tr",align:null},"no"),(0,r.kt)("td",{parentName:"tr",align:null},"python version")))),(0,r.kt)("h3",{id:"42-queue-resource-configuration"},"4.2 Queue resource configuration"),(0,r.kt)("p",null,"Because the execution of ",(0,r.kt)("inlineCode",{parentName:"p"},"spark")," requires queue resources, you need to set up a queue that you can execute.    "),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"yarn",src:n(51395).Z,width:"1252",height:"653"})," "),(0,r.kt)("h3",{id:"43-configuration-modification"},"4.3 Configuration modification"),(0,r.kt)("p",null,"If the default parameters are not satisfied, there are the following ways to configure some basic parameters"),(0,r.kt)("h4",{id:"431-management-console-configuration"},"4.3.1 Management Console Configuration"),(0,r.kt)("p",null,"Users can customize settings, such as the number of ",(0,r.kt)("inlineCode",{parentName:"p"},"spark")," sessions ",(0,r.kt)("inlineCode",{parentName:"p"},"executor")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"executor")," memory. These parameters are for users to set their own ",(0,r.kt)("inlineCode",{parentName:"p"},"spark")," parameters more freely, and other ",(0,r.kt)("inlineCode",{parentName:"p"},"spark")," parameters can also be modified, such as the ",(0,r.kt)("inlineCode",{parentName:"p"},"python")," version of ",(0,r.kt)("inlineCode",{parentName:"p"},"pyspark"),", etc.\n",(0,r.kt)("img",{alt:"spark",src:n(57055).Z,width:"1271",height:"737"})),(0,r.kt)("p",null,"Note: After modifying the configuration under the ",(0,r.kt)("inlineCode",{parentName:"p"},"IDE")," tag, you need to specify ",(0,r.kt)("inlineCode",{parentName:"p"},"-creator IDE")," to take effect (other tags are similar), such as:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},'sh ./bin/linkis-cli -creator IDE \\\n-engineType spark-2.4.3 -codeType sql \\\n-code "show databases"  \\\n-submitUser hadoop -proxyUser hadoop\n')),(0,r.kt)("h4",{id:"432-task-interface-configuration"},"4.3.2 Task interface configuration"),(0,r.kt)("p",null,"Submit the task interface, configure it through the parameter ",(0,r.kt)("inlineCode",{parentName:"p"},"params.configuration.runtime")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},'Example of http request parameters\n{\n    "executionContent": {"code": "show databases;", "runType":  "sql"},\n    "params": {\n                    "variable": {},\n                    "configuration": {\n                            "runtime": {\n                                "wds.linkis.rm.instance":"10"\n                                }\n                            }\n                    },\n    "labels": {\n        "engineType": "spark-2.4.3",\n        "userCreator": "hadoop-IDE"\n    }\n}\n')),(0,r.kt)("h3",{id:"44-engine-related-data-sheet"},"4.4 Engine related data sheet"),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"Linkis")," is managed through the engine tag, and the data table information involved is shown below."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"linkis_ps_configuration_config_key: Insert the key and default values \u200b\u200b\u200b\u200bof the configuration parameters of the engine\nlinkis_cg_manager_label: insert engine label such as: spark-2.4.3\nlinkis_ps_configuration_category: The directory association relationship of the insertion engine\nlinkis_ps_configuration_config_value: The configuration that the insertion engine needs to display\nlinkis_ps_configuration_key_engine_relation: The relationship between the configuration item and the engine\n")),(0,r.kt)("p",null,"The initial data in the table related to the ",(0,r.kt)("inlineCode",{parentName:"p"},"spark")," engine is as follows"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"-- set variable\nSET @SPARK_LABEL=\"spark-2.4.3\";\nSET @SPARK_ALL=CONCAT('*-*,',@SPARK_LABEL);\nSET @SPARK_IDE=CONCAT('*-IDE,',@SPARK_LABEL);\n\n-- engine label\ninsert into `linkis_cg_manager_label` (`label_key`, `label_value`, `label_feature`, `label_value_size`, `update_time`, `create_time`) VALUES ('combined_userCreator_engineType', @SPARK_ALL, 'OPTIONAL', 2, now(), now());\ninsert into `linkis_cg_manager_label` (`label_key`, `label_value`, `label_feature`, `label_value_size`, `update_time`, `create_time`) VALUES ('combined_userCreator_engineType', @SPARK_IDE, 'OPTIONAL', 2, now(), now());\n\nselect @label_id := id from linkis_cg_manager_label where `label_value` = @SPARK_IDE;\ninsert into linkis_ps_configuration_category (`label_id`, `level`) VALUES (@label_id, 2);\n\n-- configuration key\nINSERT INTO `linkis_ps_configuration_config_key` (`key`, `description`, `name`, `default_value`, `validate_type`, `validate_range`, `is_hidden`, `is_advanced`, `level`, `treeName`, `engine_conn_type`) VALUES ('wds.linkis.rm.instance', 'Range: 1-20, unit: each', 'Maximum concurrent number of spark engine', '10', 'NumInterval', '[1,20]', '0 ', '0', '1', 'queue resources', 'spark');\nINSERT INTO `linkis_ps_configuration_config_key` (`key`, `description`, `name`, `default_value`, `validate_type`, `validate_range`, `is_hidden`, `is_advanced`, `level`, `treeName`, `engine_conn_type`) VALUES ('spark.executor.instances', 'value range: 1-40, unit: individual', 'maximum concurrent number of spark executor instances', '1', 'NumInterval', '[1,40]', '0', '0', '2', 'spark resource settings', 'spark');\nINSERT INTO `linkis_ps_configuration_config_key` (`key`, `description`, `name`, `default_value`, `validate_type`, `validate_range`, `is_hidden`, `is_advanced`, `level`, `treeName`, `engine_conn_type`) VALUES ('spark.executor.cores', 'Value range: 1-8, unit: number', 'Number of spark executor cores', '1', 'NumInterval', '[1,8]', ' 0', '0', '1','spark resource settings', 'spark');\nINSERT INTO `linkis_ps_configuration_config_key` (`key`, `description`, `name`, `default_value`, `validate_type`, `validate_range`, `is_hidden`, `is_advanced`, `level`, `treeName`, `engine_conn_type`) VALUES ('spark.executor.memory', 'value range: 1-15, unit: G', 'spark executor memory size', '1g', 'Regex', '^([1-9]|1 [0-5])(G|g)$', '0', '0', '3', 'spark resource settings', 'spark');\nINSERT INTO `linkis_ps_configuration_config_key` (`key`, `description`, `name`, `default_value`, `validate_type`, `validate_range`, `is_hidden`, `is_advanced`, `level`, `treeName`, `engine_conn_type`) VALUES ('spark.driver.cores', 'Value range: only 1, unit: number', 'Number of spark driver cores', '1', 'NumInterval', '[1,1]', '0 ', '1', '1', 'spark resource settings', 'spark');\nINSERT INTO `linkis_ps_configuration_config_key` (`key`, `description`, `name`, `default_value`, `validate_type`, `validate_range`, `is_hidden`, `is_advanced`, `level`, `treeName`, `engine_conn_type`) VALUES ('spark.driver.memory', 'value range: 1-15, unit: G', 'spark driver memory size','1g', 'Regex', '^([1-9]|1[ 0-5])(G|g)$', '0', '0', '1', 'spark resource settings', 'spark');\nINSERT INTO `linkis_ps_configuration_config_key` (`key`, `description`, `name`, `default_value`, `validate_type`, `validate_range`, `is_hidden`, `is_advanced`, `level`, `treeName`, `engine_conn_type`) VALUES ('wds.linkis.engineconn.max.free.time', 'Value range: 3m,15m,30m,1h,2h', 'Engine idle exit time','1h', 'OFT', '[\\ \"1h\\\",\\\"2h\\\",\\\"30m\\\",\\\"15m\\\",\\\"3m\\\"]', '0', '0', '1', 'spark engine settings', ' spark');\nINSERT INTO `linkis_ps_configuration_config_key` (`key`, `description`, `name`, `default_value`, `validate_type`, `validate_range`, `is_hidden`, `is_advanced`, `level`, `treeName`, `engine_conn_type`) VALUES ('spark.tispark.pd.addresses', NULL, NULL, 'pd0:2379', 'None', NULL, '0', '0', '1', 'tidb\u8bbe\u7f6e', 'spark');\nINSERT INTO `linkis_ps_configuration_config_key` (`key`, `description`, `name`, `default_value`, `validate_type`, `validate_range`, `is_hidden`, `is_advanced`, `level`, `treeName`, `engine_conn_type`) VALUES ('spark.tispark.tidb.addr', NULL, NULL, 'tidb', 'None', NULL, '0', '0', '1', 'tidb\u8bbe\u7f6e', 'spark');\nINSERT INTO `linkis_ps_configuration_config_key` (`key`, `description`, `name`, `default_value`, `validate_type`, `validate_range`, `is_hidden`, `is_advanced`, `level`, `treeName`, `engine_conn_type`) VALUES ('spark.tispark.tidb.password', NULL, NULL, NULL, 'None', NULL, '0', '0', '1', 'tidb\u8bbe\u7f6e', 'spark');\nINSERT INTO `linkis_ps_configuration_config_key` (`key`, `description`, `name`, `default_value`, `validate_type`, `validate_range`, `is_hidden`, `is_advanced`, `level`, `treeName`, `engine_conn_type`) VALUES ('spark.tispark.tidb.port', NULL, NULL, '4000', 'None', NULL, '0', '0', '1', 'tidb\u8bbe\u7f6e', 'spark');\nINSERT INTO `linkis_ps_configuration_config_key` (`key`, `description`, `name`, `default_value`, `validate_type`, `validate_range`, `is_hidden`, `is_advanced`, `level`, `treeName`, `engine_conn_type`) VALUES ('spark.tispark.tidb.user', NULL, NULL, 'root', 'None', NULL, '0', '0', '1', 'tidb\u8bbe\u7f6e', 'spark');\nINSERT INTO `linkis_ps_configuration_config_key` (`key`, `description`, `name`, `default_value`, `validate_type`, `validate_range`, `is_hidden`, `is_advanced`, `level`, `treeName`, `engine_conn_type`) VALUES ('spark.python.version', 'Value range: python2,python3', 'python version','python2', 'OFT', '[\\\"python3\\\",\\\"python2\\\"]', ' 0', '0', '1', 'spark engine settings', 'spark');\n\n-- key engine relation\ninsert into `linkis_ps_configuration_key_engine_relation` (`config_key_id`, `engine_type_label_id`)\n(select config.id as `config_key_id`, label.id AS `engine_type_label_id` FROM linkis_ps_configuration_config_key config\nINNER JOIN linkis_cg_manager_label label ON config.engine_conn_type = 'spark' and label.label_value = @SPARK_ALL);\n\n-- engine default configuration\ninsert into `linkis_ps_configuration_config_value` (`config_key_id`, `config_value`, `config_label_id`)\n(select `relation`.`config_key_id` AS `config_key_id`, '' AS `config_value`, `relation`.`engine_type_label_id` AS `config_label_id` FROM linkis_ps_configuration_key_engine_relation relation\nINNER JOIN linkis_cg_manager_label label ON relation.engine_type_label_id = label.id AND label.label_value = @SPARK_ALL);\n\n")),(0,r.kt)("h2",{id:"5-introduction-for-data_calcdata-calculate"},"5. introduction for data_calc\uff08data calculate\uff09"),(0,r.kt)("p",null,"Spark ETL operation by parsing json, and the design:"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"data_calc",src:n(49173).Z,width:"781",height:"801"})),(0,r.kt)("p",null,"data_calc json configuration, tow mode\uff1a",(0,r.kt)("em",{parentName:"p"},"array"),"(:default) and ",(0,r.kt)("em",{parentName:"p"},"group")),(0,r.kt)("p",null,"The type of plugin: source, transformation, sink"),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"data_calc example")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'POST http://localhost:8087/api/rest_j/v1/entrance/submit\nContent-Type: application/json\nToken-Code: dss-AUTH\nToken-User: linkis\n\n{\n  "executionContent": {\n    // Code is the escaped json. See the following for detailed configuration instructions\n    "code": "{\\"plugins\\":[{\\"type\\":\\"source\\",\\"name\\":\\"jdbc\\",\\"config\\":{\\"resultTable\\":\\"spark_source_01\\",\\"url\\":\\"jdbc:mysql://localhost:3306/\\",\\"driver\\":\\"com.mysql.jdbc.Driver\\",\\"user\\":\\"xi_root\\",\\"password\\":\\"123456\\",\\"query\\":\\"select * from linkis.linkis_cg_manager_label\\",\\"options\\":{}}},{\\"type\\":\\"transformation\\",\\"name\\":\\"sql\\",\\"config\\":{\\"resultTable\\":\\"spark_transform_01\\",\\"sql\\":\\"select * from spark_source_01 limit 100\\"}},{\\"type\\":\\"sink\\",\\"name\\":\\"file\\",\\"config\\":{\\"sourceTable\\":\\"spark_transform_01\\",\\"path\\":\\"hdfs:///tmp/data/testjson\\",\\"serializer\\":\\"json\\",\\"partitionBy\\":[\\"label_key\\"],\\"saveMode\\":\\"overwrite\\"}}]}",\n    "runType": "data_calc"\n  },\n  "params": {\n    "variable": {},\n    "configuration": {\n      // Startup parameter\n      "startup": {\n        "spark.executor.memory": "1g",\n        "spark.driver.memory": "1g",\n        "spark.executor.cores": "1",\n        "spark.executor.instances": 1\n      }\n    }\n  },\n  "labels": {\n    "engineType": "spark-2.4.3",\n    "userCreator": "linkis-IDE"\n  }\n}\n')),(0,r.kt)("h3",{id:"51-mode-array"},"5.1 Mode Array"),(0,r.kt)("p",null,"Plugin has 3 field, name is the plugin's name, type is the plugin's type, config is the plugin's configuration"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json"},'{\n    "plugins": [\n        {\n            "type": "source",\n            "name": "jdbc",\n            "config": {\n                "resultTable": "spark_source_01",\n                "url": "jdbc:mysql://localhost:3306/",\n                "driver": "com.mysql.jdbc.Driver",\n                "user": "xi_root",\n                "password": "123456",\n                "query": "select * from linkis.linkis_cg_manager_label",\n                "options": {\n                }\n            }\n        },\n        {\n            "type": "transformation",\n            "name": "sql",\n            "config": {\n                "resultTable": "spark_transform_01",\n                "sql": "select * from spark_source_01 limit 100"\n            }\n        },\n        {\n            "type": "sink",\n            "name": "file",\n            "config": {\n                "sourceTable": "spark_transform_01",\n                "path": "hdfs:///tmp/data/testjson",\n                "serializer": "json",\n                "partitionBy": [\n                    "label_key"\n                ],\n                "saveMode": "overwrite"\n            }\n        }\n    ]\n}\n\n')),(0,r.kt)("h3",{id:"52-mode-group"},"5.2 Mode Group"),(0,r.kt)("p",null,"Plugin has 2 field, name is the plugin's name, config is the plugin's configuration"),(0,r.kt)("p",null,"The configuration is divided into three parts\uff1a"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"sources\uff1aConfig data source"),(0,r.kt)("li",{parentName:"ol"},"transformations\uff1aConfig data transformations"),(0,r.kt)("li",{parentName:"ol"},"sinks\uff1aConfig data sink")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json"},'{\n    "sources": [\n        {\n            "name": "jdbc",\n            "config": {\n                "resultTable": "spark_source_table_00001",\n                "url": "jdbc:mysql://localhost:3306/",\n                "driver": "com.mysql.jdbc.Driver",\n                "user": "test_db_rw",\n                "password": "123456",\n                "query": "select * from test_db.test_table",\n                "options": {\n                }\n            }\n        },\n        {\n           "name": "file",\n           "config": {\n              "resultTable": "spark_source_table_00002",\n              "path": "hdfs:///data/tmp/testfile.csv",\n              "serializer": "csv",\n              "columnNames": ["type", "name"],\n              "options": {\n              }\n           }\n        }\n    ],\n    "transformations": [\n        {\n            "name": "sql",\n            "config": {\n                "resultTable": "spark_transform_00001",\n                "sql": "select * from spark_source_table_00001 t1 join spark_source_table_00002 t2 on t1.type=t2.type where t1.id > 100 limit 100"\n            }\n        }\n    ],\n    "sinks": [\n        {\n            "name": "file",\n            "config": {\n                "sourceTable": "spark_transform_00001",\n                "path": "hdfs:///tmp/data/test_json",\n                "serializer": "json",\n                "partitionBy": [\n                    "label_key"\n                ],\n                "saveMode": "overwrite"\n            }\n        }\n    ]\n}\n')),(0,r.kt)("h3",{id:"53-plugin-type-description"},"5.3 Plugin type description"),(0,r.kt)("h4",{id:"531-source-configuration"},"5.3.1 Source configuration"),(0,r.kt)("p",null,"Corresponding data reading operations can read files from files and jdbc. * * Only the current source table can be used, and the temporary table registered before the current configuration cannot be used**"),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Public configuration")),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"field name")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"introduction")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"field type")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"required")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"default")))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"resultTable"),(0,r.kt)("td",{parentName:"tr",align:null},"register table name for ",(0,r.kt)("inlineCode",{parentName:"td"},"transform")," / ",(0,r.kt)("inlineCode",{parentName:"td"},"sink")),(0,r.kt)("td",{parentName:"tr",align:null},"String"),(0,r.kt)("td",{parentName:"tr",align:null},"Yes"),(0,r.kt)("td",{parentName:"tr",align:null},"-")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"persist"),(0,r.kt)("td",{parentName:"tr",align:null},"spark persist"),(0,r.kt)("td",{parentName:"tr",align:null},"Boolean"),(0,r.kt)("td",{parentName:"tr",align:null},"No"),(0,r.kt)("td",{parentName:"tr",align:null},"false")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"storageLevel"),(0,r.kt)("td",{parentName:"tr",align:null},"spark storageLevel"),(0,r.kt)("td",{parentName:"tr",align:null},"String"),(0,r.kt)("td",{parentName:"tr",align:null},"No"),(0,r.kt)("td",{parentName:"tr",align:null},"MEMORY_AND_DISK")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"options"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"https://spark.apache.org/docs/latest/sql-data-sources.html"},"spark official")),(0,r.kt)("td",{parentName:"tr",align:null},"Map<String, String>"),(0,r.kt)("td",{parentName:"tr",align:null},"No"),(0,r.kt)("td",{parentName:"tr",align:null})))),(0,r.kt)("h5",{id:"5311-file"},"5.3.1.1 file"),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Fields")),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"field name")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"introduction")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"field type")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"required")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"default")))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"path"),(0,r.kt)("td",{parentName:"tr",align:null},"File path, default is ",(0,r.kt)("inlineCode",{parentName:"td"},"hdfs")),(0,r.kt)("td",{parentName:"tr",align:null},"String"),(0,r.kt)("td",{parentName:"tr",align:null},"Yes"),(0,r.kt)("td",{parentName:"tr",align:null},"-")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"serializer"),(0,r.kt)("td",{parentName:"tr",align:null},"file format, default is   ",(0,r.kt)("inlineCode",{parentName:"td"},"parquet")),(0,r.kt)("td",{parentName:"tr",align:null},"String"),(0,r.kt)("td",{parentName:"tr",align:null},"Yes"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"parquet"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"columnNames"),(0,r.kt)("td",{parentName:"tr",align:null},"Mapped field name"),(0,r.kt)("td",{parentName:"tr",align:null},"String[]"),(0,r.kt)("td",{parentName:"tr",align:null},"No"),(0,r.kt)("td",{parentName:"tr",align:null},"-")))),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Exemples")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-JSON"},'{\n    "name": "file",\n    "config": {\n        "resultTable": "spark_source_table_00001",\n        "path": "hdfs:///data/tmp/test_csv_/xxx.csv",\n        "serializer": "csv",\n        "columnNames": ["id", "name"],\n        "options": {\n            "key": "value"\n        }\n    }\n}\n')),(0,r.kt)("h5",{id:"5312-jdbc"},"5.3.1.2 jdbc"),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Fields")),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"field name")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"introduction")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"field type")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"required")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"default")))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"url"),(0,r.kt)("td",{parentName:"tr",align:null},"jdbc url"),(0,r.kt)("td",{parentName:"tr",align:null},"String"),(0,r.kt)("td",{parentName:"tr",align:null},"Yes"),(0,r.kt)("td",{parentName:"tr",align:null},"-")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"driver"),(0,r.kt)("td",{parentName:"tr",align:null},"driver class"),(0,r.kt)("td",{parentName:"tr",align:null},"String"),(0,r.kt)("td",{parentName:"tr",align:null},"Yes"),(0,r.kt)("td",{parentName:"tr",align:null},"-")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"user"),(0,r.kt)("td",{parentName:"tr",align:null},"username"),(0,r.kt)("td",{parentName:"tr",align:null},"String"),(0,r.kt)("td",{parentName:"tr",align:null},"Yes"),(0,r.kt)("td",{parentName:"tr",align:null},"-")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"password"),(0,r.kt)("td",{parentName:"tr",align:null},"password"),(0,r.kt)("td",{parentName:"tr",align:null},"String"),(0,r.kt)("td",{parentName:"tr",align:null},"Yes"),(0,r.kt)("td",{parentName:"tr",align:null},"-")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"query"),(0,r.kt)("td",{parentName:"tr",align:null},"The functions used in the query must be supported by the datasource"),(0,r.kt)("td",{parentName:"tr",align:null},"String"),(0,r.kt)("td",{parentName:"tr",align:null},"Yes"),(0,r.kt)("td",{parentName:"tr",align:null},"-")))),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Exemples")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-JSON"},'{\n    "name": "jdbc",\n    "config": {\n        "resultTable": "spark_source_table_00001",\n        "url": "jdbc:mysql://localhost:3306/",\n        "driver": "com.mysql.jdbc.Driver",\n        "user": "local_root",\n        "password": "123456",\n        "query": "select a.xxx, b.xxxx from test_table where id > 100",\n        "options": {\n            "key": "value"\n        }\n    }\n}\n')),(0,r.kt)("h5",{id:"5313-managed_jdbc"},"5.3.1.3 managed_jdbc"),(0,r.kt)("p",null,"Data source configured in Linkis. The data source connection information will be obtained from linkis and converted to jdbc for execution."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Fields")),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"field name")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"introduction")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"field type")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"required")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"default")))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"datasource"),(0,r.kt)("td",{parentName:"tr",align:null},"Data source name configured in Linkis"),(0,r.kt)("td",{parentName:"tr",align:null},"String"),(0,r.kt)("td",{parentName:"tr",align:null},"Yes"),(0,r.kt)("td",{parentName:"tr",align:null},"-")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"query"),(0,r.kt)("td",{parentName:"tr",align:null},"The functions used in the query must be supported by the selected datasource"),(0,r.kt)("td",{parentName:"tr",align:null},"String"),(0,r.kt)("td",{parentName:"tr",align:null},"Yes"),(0,r.kt)("td",{parentName:"tr",align:null},"-")))),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Exemples")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-JSON"},'{\n    "name": "jdbc",\n    "config": {\n        "datasource": "mysql_test_db",\n        "query": "select a.xxx, b.xxxx from table where id > 100",\n        "options": {\n            "key": "value"\n        }\n    }\n}\n')),(0,r.kt)("h4",{id:"532-transform-configuration"},"5.3.2 Transform Configuration"),(0,r.kt)("p",null,"The logic of data transformation, ",(0,r.kt)("strong",{parentName:"p"},"All ",(0,r.kt)("inlineCode",{parentName:"strong"},"resultTable")," registered before the current configuration can be used")),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Public configuration")),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"field name")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"introduction")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"field type")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"required")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"default")))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"sourceTable"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"resultTable")," registered by the above ",(0,r.kt)("inlineCode",{parentName:"td"},"source")," / ",(0,r.kt)("inlineCode",{parentName:"td"},"transform")),(0,r.kt)("td",{parentName:"tr",align:null},"String"),(0,r.kt)("td",{parentName:"tr",align:null},"No"),(0,r.kt)("td",{parentName:"tr",align:null},"-")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"resultTable"),(0,r.kt)("td",{parentName:"tr",align:null},"Register ",(0,r.kt)("inlineCode",{parentName:"td"},"resultTable")," for the below ",(0,r.kt)("inlineCode",{parentName:"td"},"transform")," / ",(0,r.kt)("inlineCode",{parentName:"td"},"sink")),(0,r.kt)("td",{parentName:"tr",align:null},"String"),(0,r.kt)("td",{parentName:"tr",align:null},"Yes"),(0,r.kt)("td",{parentName:"tr",align:null},"-")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"persist"),(0,r.kt)("td",{parentName:"tr",align:null},"spark persist"),(0,r.kt)("td",{parentName:"tr",align:null},"Boolean"),(0,r.kt)("td",{parentName:"tr",align:null},"No"),(0,r.kt)("td",{parentName:"tr",align:null},"false")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"storageLevel"),(0,r.kt)("td",{parentName:"tr",align:null},"spark storageLevel"),(0,r.kt)("td",{parentName:"tr",align:null},"String"),(0,r.kt)("td",{parentName:"tr",align:null},"No"),(0,r.kt)("td",{parentName:"tr",align:null},"MEMORY_AND_DISK")))),(0,r.kt)("h5",{id:"5321-sql"},"5.3.2.1 sql"),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Fields")),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"field name")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"introduction")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"field type")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"required")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"default")))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"sql"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"resultTable")," registered by the above ",(0,r.kt)("inlineCode",{parentName:"td"},"source")," / ",(0,r.kt)("inlineCode",{parentName:"td"},"transform")," can be used"),(0,r.kt)("td",{parentName:"tr",align:null},"String"),(0,r.kt)("td",{parentName:"tr",align:null},"Yes"),(0,r.kt)("td",{parentName:"tr",align:null},"-")))),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Exemples")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-JSON"},'{\n    "name": "sql",\n    "config": {\n        "resultTable": "spark_transform_table_00001",\n        "sql": "select * from ods.ods_test_table as a join spark_source_table_00001 as b on a.vin=b.vin",\n        "cache": true\n    }\n}\n')),(0,r.kt)("h4",{id:"533-sink-configuration"},"5.3.3 Sink Configuration"),(0,r.kt)("p",null,"Write the ",(0,r.kt)("inlineCode",{parentName:"p"},"resultTable")," to a file or table, ",(0,r.kt)("strong",{parentName:"p"},"All ",(0,r.kt)("inlineCode",{parentName:"strong"},"resultTable")," registered before the current configuration can be used")),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Public configuration")),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"field name")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"introduction")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"field type")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"required")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"default")))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"sourceTable / sourceQuery"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"resultTable")," from ",(0,r.kt)("inlineCode",{parentName:"td"},"soruce")," / ",(0,r.kt)("inlineCode",{parentName:"td"},"transform")," or select sql, which will be written"),(0,r.kt)("td",{parentName:"tr",align:null},"String"),(0,r.kt)("td",{parentName:"tr",align:null},"No"),(0,r.kt)("td",{parentName:"tr",align:null},"sourceTable \u548c sourceQuery \u5fc5\u987b\u6709\u4e00\u4e2a\u4e0d\u4e3a\u7a7a sourceQuery \u4f18\u5148\u7ea7\u66f4\u9ad8")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"options"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"https://spark.apache.org/docs/latest/sql-data-sources.html"},"spark official")),(0,r.kt)("td",{parentName:"tr",align:null},"Map<String, String>"),(0,r.kt)("td",{parentName:"tr",align:null},"No"),(0,r.kt)("td",{parentName:"tr",align:null})),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"variables"),(0,r.kt)("td",{parentName:"tr",align:null},"Variables can be replaced in file, example: ",(0,r.kt)("inlineCode",{parentName:"td"},'dt="${day}"')),(0,r.kt)("td",{parentName:"tr",align:null},"Map<String, String>"),(0,r.kt)("td",{parentName:"tr",align:null},"No"),(0,r.kt)("td",{parentName:"tr",align:null},'{    "dt": "${day}",     "hour": "${hour}", }')))),(0,r.kt)("h5",{id:"5331-hive"},"5.3.3.1 hive"),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Fields")),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"field name")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"introduction")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"field type")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"required")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"default")))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"targetDatabase"),(0,r.kt)("td",{parentName:"tr",align:null},"The database of the table to be written"),(0,r.kt)("td",{parentName:"tr",align:null},"String"),(0,r.kt)("td",{parentName:"tr",align:null},"Yes"),(0,r.kt)("td",{parentName:"tr",align:null},"-")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"targetTable"),(0,r.kt)("td",{parentName:"tr",align:null},"The table to be written"),(0,r.kt)("td",{parentName:"tr",align:null},"String"),(0,r.kt)("td",{parentName:"tr",align:null},"Yes"),(0,r.kt)("td",{parentName:"tr",align:null},"-")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"saveMode"),(0,r.kt)("td",{parentName:"tr",align:null},"Write mode, refer to Spark, the default is ",(0,r.kt)("inlineCode",{parentName:"td"},"overwrite")),(0,r.kt)("td",{parentName:"tr",align:null},"String"),(0,r.kt)("td",{parentName:"tr",align:null},"Yes"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"overwrite"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"strongCheck"),(0,r.kt)("td",{parentName:"tr",align:null},"field name, field order, field type must be same"),(0,r.kt)("td",{parentName:"tr",align:null},"Boolean"),(0,r.kt)("td",{parentName:"tr",align:null},"No"),(0,r.kt)("td",{parentName:"tr",align:null},"true")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"writeAsFile"),(0,r.kt)("td",{parentName:"tr",align:null},"Write as a file, it can improve efficiency. All partition variables should be in ",(0,r.kt)("inlineCode",{parentName:"td"},"variables")),(0,r.kt)("td",{parentName:"tr",align:null},"Boolean"),(0,r.kt)("td",{parentName:"tr",align:null},"No"),(0,r.kt)("td",{parentName:"tr",align:null},"false")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"numPartitions"),(0,r.kt)("td",{parentName:"tr",align:null},"Number of partitions, ",(0,r.kt)("inlineCode",{parentName:"td"},"Dataset.repartition")),(0,r.kt)("td",{parentName:"tr",align:null},"Integer"),(0,r.kt)("td",{parentName:"tr",align:null},"No"),(0,r.kt)("td",{parentName:"tr",align:null},"10")))),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Exemples")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-JSON"},'{\n    "name": "hive",\n    "config": {\n        "sourceTable": "spark_transform_table_00001",\n        "targetTable": "dw.result_table",\n        "saveMode": "append",\n        "options": {\n            "key": "value"\n        }\n    }\n}\n')),(0,r.kt)("h5",{id:"5332-jdbc"},"5.3.3.2 jdbc"),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Fields")),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"field name")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"introduction")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"field type")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"required")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"default")))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"url"),(0,r.kt)("td",{parentName:"tr",align:null},"jdbc url"),(0,r.kt)("td",{parentName:"tr",align:null},"String"),(0,r.kt)("td",{parentName:"tr",align:null},"Yes"),(0,r.kt)("td",{parentName:"tr",align:null},"-")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"driver"),(0,r.kt)("td",{parentName:"tr",align:null},"driver class\uff08fully-qualified name\uff09"),(0,r.kt)("td",{parentName:"tr",align:null},"String"),(0,r.kt)("td",{parentName:"tr",align:null},"Yes"),(0,r.kt)("td",{parentName:"tr",align:null},"-")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"user"),(0,r.kt)("td",{parentName:"tr",align:null},"username"),(0,r.kt)("td",{parentName:"tr",align:null},"String"),(0,r.kt)("td",{parentName:"tr",align:null},"Yes"),(0,r.kt)("td",{parentName:"tr",align:null},"-")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"password"),(0,r.kt)("td",{parentName:"tr",align:null},"password"),(0,r.kt)("td",{parentName:"tr",align:null},"String"),(0,r.kt)("td",{parentName:"tr",align:null},"Yes"),(0,r.kt)("td",{parentName:"tr",align:null},"-")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"targetDatabase"),(0,r.kt)("td",{parentName:"tr",align:null},"The database of the table to be written"),(0,r.kt)("td",{parentName:"tr",align:null},"String"),(0,r.kt)("td",{parentName:"tr",align:null},"No"),(0,r.kt)("td",{parentName:"tr",align:null},"-")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"targetTable"),(0,r.kt)("td",{parentName:"tr",align:null},"The table to be written"),(0,r.kt)("td",{parentName:"tr",align:null},"String"),(0,r.kt)("td",{parentName:"tr",align:null},"Yes"),(0,r.kt)("td",{parentName:"tr",align:null},"-")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"preQueries"),(0,r.kt)("td",{parentName:"tr",align:null},"SQL executed before writing"),(0,r.kt)("td",{parentName:"tr",align:null},"String[]"),(0,r.kt)("td",{parentName:"tr",align:null},"No"),(0,r.kt)("td",{parentName:"tr",align:null},"-")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"numPartitions"),(0,r.kt)("td",{parentName:"tr",align:null},"Number of partitions, ",(0,r.kt)("inlineCode",{parentName:"td"},"Dataset.repartition")),(0,r.kt)("td",{parentName:"tr",align:null},"Integer"),(0,r.kt)("td",{parentName:"tr",align:null},"No"),(0,r.kt)("td",{parentName:"tr",align:null},"10")))),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Exemples")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-JSON"},'{\n    "name": "jdbc",\n    "config": {\n        "sourceTable": "spark_transform_table_00001",\n        "database": "mysql_test_db",\n        "targetTable": "test_001",\n        "preQueries": ["delete from test_001 where dt=\'${exec_date}\'"],\n        "options": {\n            "key": "value"\n        }\n    }\n}\n')),(0,r.kt)("h5",{id:"5333-managed_jdbc"},"5.3.3.3 managed_jdbc"),(0,r.kt)("p",null,"Data source configured in Linkis. The data source connection information will be obtained from linkis and converted to jdbc for execution."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Fields")),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"field name")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"introduction")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"field type")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"required")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"default")))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"targetDatasource"),(0,r.kt)("td",{parentName:"tr",align:null},"Data source name configured in Linkis"),(0,r.kt)("td",{parentName:"tr",align:null},"String"),(0,r.kt)("td",{parentName:"tr",align:null},"No"),(0,r.kt)("td",{parentName:"tr",align:null},"-")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"targetDatabase"),(0,r.kt)("td",{parentName:"tr",align:null},"The database of the table to be written"),(0,r.kt)("td",{parentName:"tr",align:null},"String"),(0,r.kt)("td",{parentName:"tr",align:null},"No"),(0,r.kt)("td",{parentName:"tr",align:null},"-")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"targetTable"),(0,r.kt)("td",{parentName:"tr",align:null},"The table to be written"),(0,r.kt)("td",{parentName:"tr",align:null},"String"),(0,r.kt)("td",{parentName:"tr",align:null},"Yes"),(0,r.kt)("td",{parentName:"tr",align:null},"-")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"preQueries"),(0,r.kt)("td",{parentName:"tr",align:null},"SQL executed before writing"),(0,r.kt)("td",{parentName:"tr",align:null},"String[]"),(0,r.kt)("td",{parentName:"tr",align:null},"No"),(0,r.kt)("td",{parentName:"tr",align:null},"-")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"numPartitions"),(0,r.kt)("td",{parentName:"tr",align:null},"Number of partitions, ",(0,r.kt)("inlineCode",{parentName:"td"},"Dataset.repartition")),(0,r.kt)("td",{parentName:"tr",align:null},"Integer"),(0,r.kt)("td",{parentName:"tr",align:null},"No"),(0,r.kt)("td",{parentName:"tr",align:null},"10")))),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Exemples")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-JSON"},'{\n    "name": "jdbc",\n    "config": {\n        "targetDatasource": "spark_transform_table_00001",\n        "targetDatabase": "mysql_test_db",\n        "targetTable": "test_001",\n        "preQueries": ["delete from test_001 where dt=\'${exec_date}\'"],\n        "options": {\n            "key": "value"\n        }\n    }\n}\n')),(0,r.kt)("h5",{id:"5334-file"},"5.3.3.4 file"),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Fields")),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"field name")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"introduction")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"field type")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"required")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"default")))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"path"),(0,r.kt)("td",{parentName:"tr",align:null},"File path, default is ",(0,r.kt)("inlineCode",{parentName:"td"},"hdfs")),(0,r.kt)("td",{parentName:"tr",align:null},"String"),(0,r.kt)("td",{parentName:"tr",align:null},"Yes"),(0,r.kt)("td",{parentName:"tr",align:null},"-")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"serializer"),(0,r.kt)("td",{parentName:"tr",align:null},"file format, default is   ",(0,r.kt)("inlineCode",{parentName:"td"},"parquet")),(0,r.kt)("td",{parentName:"tr",align:null},"String"),(0,r.kt)("td",{parentName:"tr",align:null},"Yes"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"parquet"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"partitionBy"),(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null},"String[]"),(0,r.kt)("td",{parentName:"tr",align:null},"No"),(0,r.kt)("td",{parentName:"tr",align:null})),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"saveMode"),(0,r.kt)("td",{parentName:"tr",align:null},"Write mode, refer to Spark, the default is ",(0,r.kt)("inlineCode",{parentName:"td"},"overwrite")),(0,r.kt)("td",{parentName:"tr",align:null},"String"),(0,r.kt)("td",{parentName:"tr",align:null},"No"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"overwrite"))))),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Exemples")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-JSON"},'{\n    "name": "file",\n    "config": {\n        "sourceTable": "spark_transform_table_00001",\n        "path": "hdfs:///data/tmp/test_json/",\n        "serializer": "json", \n        "variables": {\n            "key": "value"\n        },\n        "options": {\n            "key": "value"\n        }\n    }\n}\n')),(0,r.kt)("h3",{id:"54-exemples"},"5.4 Exemples"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"Read from jdbc, write to hive")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json"},'{\n    "sources": [\n        {\n            "name": "jdbc",\n            "config": {\n                "resultTable": "spark_source_0001",\n                "database": "mysql_test_db",\n                "query": "select * from test_table limit 100"\n            }\n        }\n    ],\n    "transformations": [\n    ],\n    "sinks": [\n        {\n            "name": "hive",\n            "config": {\n                "sourceTable": "spark_source_0001",\n                "targetTable": "ods.ods_test_table",\n                "saveMode": "overwrite"\n            }\n        }\n    ]\n}\n')),(0,r.kt)("ol",{start:2},(0,r.kt)("li",{parentName:"ol"},"Read from hive, write to hive")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json"},'{\n    "sources": [\n    ],\n    "transformations": [\n        {\n            "name": "sql",\n            "config": {\n                "resultTable": "spark_transform_00001",\n                "sql": "select * from ods.ods_test_table limit 100"\n            }\n        }\n    ],\n    "sinks": [\n        {\n            "name": "hive",\n            "config": {\n                "sourceTable": "spark_transform_00001",\n                "targetTable": "dw.dw_test_table",\n                "saveMode": "overwrite"\n            }\n        }\n    ]\n}\n')),(0,r.kt)("ol",{start:3},(0,r.kt)("li",{parentName:"ol"},"Read from file, write to hive")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json"},'{\n    "sources": [\n        {\n            "name": "file",\n            "config": {\n                "path": "hdfs:///data/tmp/test_csv/",\n                "resultTable": "spark_file_0001",\n                "serializer": "csv",\n                "columnNames": ["col1", "col2", "col3"],\n                "options": {\n                    "delimiter": ",",\n                    "header", "false"\n                }\n            }\n        }\n    ],\n    "transformations": [\n    ],\n    "sinks": [\n        {\n            "name": "hive",\n            "config": {\n                "sourceQuery": "select col1, col2 from spark_file_0001",\n                "targetTable": "ods.ods_test_table",\n                "saveMode": "overwrite",\n                "variables": {\n                }\n            }\n        }\n    ]\n}\n')),(0,r.kt)("ol",{start:4},(0,r.kt)("li",{parentName:"ol"},"Read from hive, write to jdbc")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json"},'{\n    "sources": [\n    ],\n    "transformations": [\n    ],\n    "sinks": [\n        {\n            "name": "jdbc",\n            "config": {\n                "sourceQuery": "select * from dm.dm_result_table where dt=${exec_date}",\n                "database": "mysql_test_db",\n                "targetTable": "mysql_test_table",\n                "preQueries": ["delete from mysql_test_table where dt=\'${exec_date}\'"],\n                "options": {\n                    "key": "value"\n                }\n            }\n        }\n    ]\n}\n')))}u.isMDXComponent=!0},57055:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/spark-conf-5a082f340d4291dae5670ee961b95538.png"},51395:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/yarn-conf-3611575997ffb7ba32993da83d626e72.png"},49173:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/data_calc-6e8ddda2d7a9becb58240e7c09c7f70b.svg"}}]);